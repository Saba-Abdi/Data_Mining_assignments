# -*- coding: utf-8 -*-
"""HW2-Group9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wr2sGBztOuox7wiDpa5vKiZtZ3iXlwKp

### Section 2

#### Question 1
"""

# Initially, we import everyday libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Before anything else, we read the dataset.
sample_df = pd.read_csv('Sample.csv')

sample_df.info()

sample_df.head()

"""#### Question 2"""

X = sample_df['X']
y_bar = sample_df['y']

# Before binning, we depict scatter plot to have an overview of the data.
plt.figure(figsize=(6, 4))
plt.scatter(X, y_bar)
plt.xlabel('X')
plt.ylabel('y')
plt.title('Scatter Plot of X and y bar')
plt.show()

# Generating a copy to do all calculations and transformations needed.
sample_df_copy1 = sample_df.copy()
sample_df_copy1 = sample_df_copy1.sort_values(by='X')
sample_df_copy1.reset_index(drop=True, inplace=True)

# Then we use square transformation.
sample_df_copy1['y'] = (sample_df_copy1['y'])**2
sample_df_copy1.head()

# Generating a copy to do all calculations and transformations needed.
sample_df_copy2 = sample_df.copy()
sample_df_copy2 = sample_df_copy2.sort_values(by='X')
sample_df_copy2.reset_index(drop=True, inplace=True)

# Then we use square transformation.
sample_df_copy2['y'] = (sample_df_copy2['y'])**3
sample_df_copy2.head()

# Generating a copy to do all calculations and transformations neede.
sample_df_copy3 = sample_df.copy()
sample_df_copy3 = sample_df_copy3.sort_values(by='X')
sample_df_copy3.reset_index(drop=True, inplace=True)

# Then we use square transformation.
sample_df_copy3['y'] = np.sqrt(sample_df_copy3['y'])
sample_df_copy3.head()

# Generating a copy to do all calculations and transformations neede.
sample_df_copy4 = sample_df.copy()
sample_df_copy4 = sample_df_copy4.sort_values(by='X')
sample_df_copy4.reset_index(drop=True, inplace=True)

# Then we use square transformation.
sample_df_copy4['y'] = np.exp(sample_df_copy4['y'])
sample_df_copy4.head()

# Generating a copy to do all calculations and transformations neede.
sample_df_copy5 = sample_df.copy()
sample_df_copy5 = sample_df_copy5.sort_values(by='X')
sample_df_copy5.reset_index(drop=True, inplace=True)

# Then we use square transformation.
sample_df_copy5['y'] = np.cbrt(sample_df_copy5['y'])
sample_df_copy5.head()

# In order to know each bin better, we draw scatter plot of each of the bins.
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(8, 10))
axes = axes.flatten()

axes[0].scatter(X, y_bar)
axes[0].set_xlabel('X')
axes[0].set_ylabel('y')
axes[0].set_title('Scatter Plot of X and y  ')

axes[1].scatter(sample_df_copy1['X'], sample_df_copy1['y'])
axes[1].set_xlabel('X')
axes[1].set_ylabel('y')
axes[1].set_title('Scatter Plot of X and y - y^(2) ')

axes[2].scatter(sample_df_copy2['X'], sample_df_copy2['y'])
axes[2].set_xlabel('X')
axes[2].set_ylabel('y')
axes[2].set_title('Scatter Plot of X and y - y^(3) ')

axes[3].scatter(sample_df_copy3['X'], sample_df_copy3['y'])
axes[3].set_xlabel('X')
axes[3].set_ylabel('y')
axes[3].set_title('Scatter Plot of X and y - sqrt of y ')

axes[4].scatter(sample_df_copy4['X'], sample_df_copy4['y'])
axes[4].set_xlabel('X')
axes[4].set_ylabel('y')
axes[4].set_title('Scatter Plot of X and y - exp of y ')

axes[5].scatter(sample_df_copy5['X'], sample_df_copy5['y'])
axes[5].set_xlabel('X')
axes[5].set_ylabel('y')
axes[5].set_title('Scatter Plot of X and y - cbrt of y ')


plt.tight_layout()
plt.show()

bin1 = sample_df_copy3[(sample_df_copy3['y'] >= 1.97) & (sample_df_copy3['X'] <= 45) ]

X_bin1 = bin1['X'].values.reshape(-1, 1)
y_bar_bin1 = bin1['y'].values.reshape(-1, 1)

# Constructing the model.
model_bin1 = LinearRegression()
model_bin1.fit(X_bin1, y_bar_bin1)
# Prediction process
y_prediction_bin1 = model_bin1.predict(X_bin1)
# R2 calculation
r2_bin1 = r2_score(y_bar_bin1, y_prediction_bin1)
print("R^2 for Bin 1:", r2_bin1)

plt.figure(figsize=(4, 3))
plt.scatter(X_bin1, y_bar_bin1, label="Bin 1", alpha=0.6)
plt.plot(X_bin1, y_prediction_bin1, linestyle="--", color="black")
plt.title("Linear Regression for Bin 1")
plt.xlabel("X")
plt.ylabel("y values of Bin 1")
plt.show()

bin2 = sample_df_copy3[(sample_df_copy3['X'] > 45) & (sample_df_copy3['y'] > 1.7)]

X_bin2 = bin2['X'].values.reshape(-1, 1)
y_bar_bin2 = bin2['y'].values.reshape(-1, 1)

# Constructing the model.
model_bin2 = LinearRegression()
model_bin2.fit(X_bin2, y_bar_bin2)
# Prediction process
y_prediction_bin2 = model_bin2.predict(X_bin2)
# R2 calculation
r2_bin2 = r2_score(y_bar_bin2, y_prediction_bin2)
print("R^2 for Bin 2:", r2_bin2)

plt.figure(figsize=(4, 3))
plt.scatter(X_bin2, y_bar_bin2, label="Bin 2", alpha=0.6)
plt.plot(X_bin2, y_prediction_bin2, linestyle="--", color="black")
plt.title("Linear Regression for Bin 2")
plt.xlabel("X")
plt.ylabel("y values of Bin 2")
plt.show()

bin3 = sample_df_copy4[ (sample_df_copy4['X'] <= 20) & (sample_df_copy4['y'] < 27)]
new_df = sample_df_copy4[ (sample_df_copy4['X'] <= 40) & (sample_df_copy4['y'] < 50) & (sample_df_copy4['X'] > 20)]
bin3 = pd.concat([bin3, new_df], ignore_index=True)


X_bin3 = bin3['X'].values.reshape(-1, 1)
y_bar_bin3 = bin3['y'].values.reshape(-1, 1)

# Constructing the model.
model_bin3 = LinearRegression()
model_bin3.fit(X_bin3, y_bar_bin3)
# Prediction process
y_prediction_bin3 = model_bin3.predict(X_bin3)
# R2 calculation
r2_bin3 = r2_score(y_bar_bin3, y_prediction_bin3)
print("R^2 for Bin 3:", r2_bin3)

plt.figure(figsize=(4, 3))
plt.scatter(X_bin3, y_bar_bin3, label="Bin 3", alpha=0.6)
plt.plot(X_bin3, y_prediction_bin3, linestyle="--", color="black")
plt.title("Linear Regression for Bin 3")
plt.xlabel("X")
plt.ylabel("y values of Bin 3")
plt.show()

bin4 = sample_df_copy4[ (sample_df_copy4['X'] < 60) & (sample_df_copy4['X'] > 40) & (sample_df_copy4['y'] <24)]
new_df2 = sample_df_copy4[ (sample_df_copy4['X'] < 21) & (sample_df_copy4['y'] > 26) & (sample_df_copy4['y'] <45)]
bin4 = pd.concat([bin4, new_df2], ignore_index=True)

x_bin4 = bin4['X'].values.reshape(-1, 1)
y_bar_bin4 = bin4['y'].values.reshape(-1, 1)

model_bin4 = LinearRegression()
model_bin4.fit(x_bin4, y_bar_bin4)
y_prediction_bin4 = model_bin4.predict(x_bin4)
r2_bin4 = r2_score(y_bar_bin4, y_prediction_bin4)
print("R^2 for Bin 4:", r2_bin4)

plt.figure(figsize=(4, 3))
plt.scatter(x_bin4, y_bar_bin4, label="Bin 4", alpha=0.6)
plt.plot(x_bin4, y_prediction_bin4, linestyle="--", color="black")
plt.title("Linear Regression for Bin 4")
plt.xlabel("X")
plt.ylabel("y values of Bin 4")
plt.show()

"""### Section 3

#### Question 1
"""

from scipy.stats import chi2_contingency
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

df = pd.read_csv('Question1-Diabets.csv')
df.head()

figsize = (12, 1.2 * len(df['Diabetes_Status'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(df, x='Glucose_Level', y='Diabetes_Status', inner='stick', palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)
plt.show()

def chi_merge(df, col_name, target_col, max_intervals=3, threshold=None):
    import pandas as pd
    from scipy.stats import chi2_contingency

    df = df.sort_values(by=col_name).reset_index(drop=True)
    df[target_col] = df[target_col].map({'Yes': 1, 'No': 0})

    # ÿ¥ÿ±Ÿàÿπ ÿ®ÿß Ÿáÿ± ŸÖŸÇÿØÿßÿ± €å⁄©ÿ™ÿß ÿ®Ÿá‚ÄåÿπŸÜŸàÿßŸÜ €å⁄© ÿ®ÿßÿ≤Ÿá ÿ¨ÿØÿß
    unique_vals = df[col_name].unique()
    intervals = [[val] for val in unique_vals]

    def compute_chi(group1, group2):
        data = df[df[col_name].isin(group1 + group2)]
        table = pd.crosstab(data[col_name], data[target_col])
        chi, _, _, _ = chi2_contingency(table)
        return chi

    while len(intervals) > max_intervals:
        min_chi, min_index = float('inf'), None

        for i in range(len(intervals) - 1):
            chi = compute_chi(intervals[i], intervals[i + 1])
            if chi < min_chi:
                min_chi = chi
                min_index = i

        if threshold and min_chi > threshold:
            break

        # ÿßÿØÿ∫ÿßŸÖ ÿ®ÿßÿ≤Ÿá‚ÄåŸáÿß
        intervals[min_index] += intervals[min_index + 1]
        del intervals[min_index + 1]

    # ÿ™ÿπ€å€åŸÜ ŸÖÿ±ÿ≤Ÿáÿß€å Ÿáÿ± ÿ®ÿßÿ≤Ÿá
    bounds = [(min(g), max(g)) for g in intervals]

    def categorize(val):
        for i, group in enumerate(intervals):
            if val in group:
                return ['⁄©ŸÖ', 'ŸÖÿ™Ÿàÿ≥ÿ∑', 'ÿ≤€åÿßÿØ'][i]
        return 'ŸÜÿßŸÖÿ¥ÿÆÿµ'

    df['Glucose_Category'] = df[col_name].apply(categorize)

    print("ŸÖŸÇÿßÿØ€åÿ± ŸÖÿ±ÿ≤€å ÿ®ÿ±ÿß€å Ÿáÿ± ÿ®ÿßÿ≤Ÿá:")
    for i, (lo, hi) in enumerate(bounds):
        print(f"ÿ®ÿßÿ≤Ÿá {['⁄©ŸÖ', 'ŸÖÿ™Ÿàÿ≥ÿ∑', 'ÿ≤€åÿßÿØ'][i]}: {lo} ÿ™ÿß {hi}")

    return df


# ÿßÿ¨ÿ±ÿß ÿ±Ÿà€å ÿØÿßÿØŸá
df = chi_merge(df, col_name='Glucose_Level', target_col='Diabetes_Status', max_intervals=3)
print(df[['Glucose_Level', 'Glucose_Category']])

"""#### Question 2"""

# loading and reading the data
cleaning_df = pd.read_excel('Question2-Cleaning.xlsx')
cleaning_df.head()

# First and foremost, we change names of the columns
cleaning_df.columns = ['id', 'age', 'annual_salary', 'additional_monetary_compensation', 'currency', 'country', 'work_experience', 'field_experience', 'education', 'gender']
cleaning_df.head()

# getting information to know the data better
cleaning_df.info()

import pandas as pd

df_cleaned = pd.read_excel('Question2-Cleaning.xlsx')

df_cleaned.columns = [
    "ID", "Age", "Annual_Salary", "Additional_Compensation", "Currency",
    "Country", "Total_Experience", "Field_Experience", "Education", "Gender"
]

df_cleaned.dropna(subset=["Annual_Salary", "Country", "Age", "Currency"], inplace=True)

df_cleaned["Country"] = df_cleaned["Country"].str.upper().str.strip()
df_cleaned["Gender"] = df_cleaned["Gender"].str.strip().str.title()
df_cleaned["Additional_Compensation"] = df_cleaned["Additional_Compensation"].fillna(0)

q_low = df_cleaned["Annual_Salary"].quantile(0.01)
q_high = df_cleaned["Annual_Salary"].quantile(0.99)
df_cleaned = df_cleaned[(df_cleaned["Annual_Salary"] >= q_low) & (df_cleaned["Annual_Salary"] <= q_high)]

df_cleaned["Total_Experience"] = df_cleaned["Total_Experience"].str.strip()
df_cleaned["Field_Experience"] = df_cleaned["Field_Experience"].str.strip()

"""ÿ™ŸÖ€åÿ≤ ⁄©ÿ±ÿØŸÜ ÿ≥ÿ™ŸàŸÜ ⁄©ÿ¥Ÿàÿ±"""

df_cleaned['Country'] = df_cleaned['Country'].str.strip().str.upper()
df_cleaned['Country'] = df_cleaned['Country'].replace({
    'US': 'USA',
    'UNITED STATES OF AMERICA': 'USA',
    'UNITED STATES': 'USA',
    'U.S.': 'USA',
    'UNITES STATES': 'USA',
    'U.S.A.': 'USA',
    'UNITE STATES': 'USA',
    'THE UNITED STATES': 'USA',
    'THE US': 'USA',
    'U.S': 'USA',
    'UNITED STATE': 'USA',
    'UNITED STATEA': 'USA',
    'U. S.': 'USA',
    'UNITED STATEW': 'USA',
    'UNITED SATES' : 'USA',
    'UNTIED STATES' : 'USA',
    'UNITIED STATES' : 'USA',
    'U. S' : 'USA',
    'UNITED STATSS' : 'USA',
    'U.S.A' : 'USA',
    'UNITED STATUS' : 'USA',
    'üá∫üá∏' : 'USA',
    'UNITED STATEES' : 'USA',
    'UNIYED STATES' : 'USA',
    'UNTED STATES' : 'USA',
    'UNITEED STATES' : 'USA',
    'UNITED STATED' : 'USA',
    'UNITED STARES' : 'USA',
    'UNITED STATESP' : 'USA',
    'UNITED STATES OF AMERICAS' : 'USA',
    'U.SA' : 'USA',
    'UNITED STATUES' : 'USA',
    'UNITED STTES' : 'USA',
    'UNITER STATEZ' : 'USA',
    'USAT' : 'USA',
    'U.S>' : 'USA',
    'UNITED STATEDS' : 'USA',
    'UNITED STATWS' : 'USA',
    'AMERICA' : 'USA',
    'UNIYES STATES' : 'USA',
    'UNITED  STATES' : 'USA',
    'UNITEDSTATES' : 'USA',
    'UNITEF STATED' : 'USA',
    'UNITED STATTES': 'USA'

})

df_cleaned['Country'] = df_cleaned['Country'].replace({
    'UNITED KINGDOM': 'UK',
    'U.K': 'UK',
    'U.K.': 'UK',
    'UNITED KINDOM': 'UK',
    'ENGLAND': 'UK',
    'U.K. (NORTHERN ENGLAND)': 'UK',
    'ENGLAND, UK': 'UK',
    'ENGLAND, UNITED KINGDOM': 'UK',
    'UNITED KINGDOM (ENGLAND)': 'UK',
    'UNITED KINGDOM.': 'UK',
    'ENGLAND, GB' : 'UK',
    'GREAT BRITAIN' : 'UK',
    'BRITAIN' : 'UK',
    'UK FOR U.S. COMPANY' : 'UK',
    'ENGLAND/UK': 'UK'
})

df_cleaned['Country'] = df_cleaned['Country'].replace({
    'CANAD': 'CANADA',
    'CANADA, OTTAWA, ONTARIO': 'CANADA',
    'CANDA': 'CANADA',
    'CAN': 'CANADA',
    'CANDA': 'CANADA',
    'CANAD√Å': 'CANADA',
    'CANADW': 'CANADA'
})

df_cleaned['Country'] = df_cleaned['Country'].replace({
    'THE NETHERLANDS': 'NETHERLANDS',
    'NEDERLAND' : 'NETHERLANDS'
})

df_cleaned['Country'] = df_cleaned['Country'].replace({
    'NEW ZEALAND AOTEAROA': 'NEW ZEALAND'
})

df_cleaned['Country'] = df_cleaned['Country'].replace({
    'ITALIA': 'ITALY'
})

df_cleaned['Country'] = df_cleaned['Country'].replace({
    'DANMARK': 'DENMARK'
})

df_cleaned['Country'] = df_cleaned['Country'].replace({
    'U.A.': 'UNITED ARAB EMIRATES'
})

df_cleaned['Country'] = df_cleaned['Country'].replace({
    'UA': 'UKRAINE'
})

df_cleaned['Country'] = df_cleaned['Country'].replace({
    'JAPAN, US GOV POSITION': 'JAPAN'
})

df_cleaned['Country'] = df_cleaned['Country'].replace({
    'AUSTRALIAN': 'AUSTRALIA',
    'AUSTRALI': 'AUSTRALIA',
    'CANDA': 'CANADA'
})

df_cleaned = df_cleaned.drop(df_cleaned[df_cleaned['Country'] == 'CANADA AND USA'].index)



df_cleaned['Country'].unique()

"""ÿ™ŸÖ€åÿ≤⁄©ÿßÿ±€å ÿ≥ÿ™ŸàŸÜ ÿ¨ŸÜÿ≥€åÿ™"""

df_cleaned['Gender'] = df_cleaned['Gender'].replace({
    'Man': 'Male',
    'Woman': 'Female',
    'Other Or Prefer Not To Answer': 'Other',
    'Prefer Not To Answer': 'Other'
})

df_cleaned['Gender'] = df_cleaned['Gender'].fillna('Other')
df_cleaned['Gender'].unique()

"""ÿ™ŸÖ€åÿ≤⁄©ÿßÿ±€å ÿ≥ÿß€åÿ± ÿ≥ÿ™ŸàŸÜ Ÿáÿß"""

df_cleaned['Total_Experience'] = df_cleaned['Total_Experience'].replace({
    '3 - 4 years': '2 - 4 years',
    '4 - 4 years': '2 - 4 years',

})

df_cleaned['Total_Experience'] = df_cleaned['Total_Experience'].fillna('1 year or less')
df_cleaned['Total_Experience'].unique()

"""ÿ®ÿ±ÿ±ÿ≥€å ÿØÿ≥ÿ™€å ŸÖŸàÿßÿ±ÿØ ÿπÿ¨€åÿ®"""

df_cleaned['Field_Experience'] = df_cleaned['Field_Experience'].fillna('1 year or less')

df_cleaned[(df_cleaned['Age'] == 'under 18') & (df_cleaned['Education'] == 'PhD')]
df_cleaned.loc[df_cleaned['ID'] == '12087un', 'Age'] = '25-34'
df_cleaned.loc[df_cleaned['ID'] == '2518418', 'Age'] = '55-64'
df_cleaned.loc[df_cleaned['ID'] == '1076318', 'Age'] = '55-64'
df_cleaned.loc[df_cleaned['ID'] == '2573518', 'Age'] = '45-54'
df_cleaned.loc[df_cleaned['ID'] == '2463un', 'Age'] = '45-54'
df_cleaned.loc[df_cleaned['ID'] == '16282un', 'Education'] = 'High School'

"""ÿ®ÿ±ÿ±ÿ≥€å ÿÆÿßŸÜŸá Ÿáÿß€å ÿÆÿßŸÑ€å ÿ®ÿßŸÇ€å ŸÖÿßŸÜÿØŸá"""

df_cleaned['Education'].isna()

"""ÿ≠ÿØÿ≥ Ÿà Ÿæ€åÿ¥ÿ®€åŸÜ€å ŸÖÿØÿ±⁄© ÿ™ÿ≠ÿµ€åŸÑ€å ÿ´ÿ®ÿ™ ŸÜÿ¥ÿØŸá"""

df_known = df_cleaned[df_cleaned['Education'].notna()]
df_missing = df_cleaned[df_cleaned['Education'].isna()]

from sklearn.preprocessing import LabelEncoder

df_model = df_cleaned.copy()
le_age = LabelEncoder()
le_experience = LabelEncoder()

df_model['Age_encoded'] = le_age.fit_transform(df_model['Age'].astype(str))
df_model['Experience_encoded'] = le_experience.fit_transform(df_model['Total_Experience'].astype(str))

features = ['Age_encoded', 'Annual_Salary', 'Additional_Compensation', 'Experience_encoded']

from sklearn.ensemble import RandomForestClassifier

X_train = df_model.loc[df_model['Education'].notna(), features]
y_train = df_model.loc[df_model['Education'].notna(), 'Education']

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

X_missing = df_model.loc[df_model['Education'].isna(), features]
predicted_education = model.predict(X_missing)

df_cleaned.loc[df_cleaned['Education'].isna(), 'Education'] = predicted_education

"""ŸáŸàÿ±ÿßÿß!!"""

df_cleaned.isna().sum()

df_cleaned.to_csv('cleaned_data.csv', index=False)

